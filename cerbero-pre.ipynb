{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz52EJZsVqRwdMxjfEM1lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4L3M4R/cerbero/blob/main/cerbero-pre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYcC5-caMLbD"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "#             CERBERO PREMARKET\n",
        "#   Descarga noticias y calcula sentiment\n",
        "#   (Actualiza el día anterior en los archivos *_datos.txt)\n",
        "# ===============================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote\n",
        "\n",
        "# ===============================================\n",
        "#             CONFIGURACIÓN\n",
        "# ===============================================\n",
        "\n",
        "# Lista de activos\n",
        "activos = {}\n",
        "with open(\"activos.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        symbol, source, search_name = line.strip().split(\":\")\n",
        "        activos[symbol.strip()] = {\n",
        "            \"source\": source.strip().lower(),\n",
        "            \"search_name\": search_name.strip()\n",
        "        }\n",
        "\n",
        "# Inicialización de analizadores de sentimiento\n",
        "nltk.download('vader_lexicon')\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "finbert_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "# ===============================================\n",
        "#           FUNCIONES AUXILIARES\n",
        "# ===============================================\n",
        "\n",
        "def registrar_log(message, log_file=\"run_summary_pre.log\"):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(log_file, \"a\") as log:\n",
        "        log.write(f\"[{timestamp}] {message}\\n\")\n",
        "\n",
        "def descargar_noticias_y_calcular_sentiment_df(df, symbol, search_name):\n",
        "    \"\"\"\n",
        "    Descarga noticias, calcula el sentiment y actualiza el DataFrame pasado.\n",
        "    Devuelve el DataFrame actualizado sin guardar todavía en disco.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "    from urllib.parse import quote\n",
        "    import feedparser\n",
        "    import pandas as pd\n",
        "\n",
        "    query = quote(search_name)\n",
        "    feed = feedparser.parse(f\"https://news.google.com/rss/search?q={query}\")\n",
        "    noticias = []\n",
        "    for entry in feed.entries:\n",
        "        noticias.append({\n",
        "            \"timestamp\": entry.published,\n",
        "            \"title\": entry.title,\n",
        "            \"link\": entry.link\n",
        "        })\n",
        "\n",
        "    noticias_df = pd.DataFrame(noticias)\n",
        "    if noticias_df.empty:\n",
        "        registrar_log(f\"{symbol} - No se encontraron noticias\")\n",
        "        return df\n",
        "\n",
        "    # Calcular sentimiento con VADER\n",
        "    noticias_df[\"vader_sentiment\"] = noticias_df[\"title\"].apply(lambda x: vader_analyzer.polarity_scores(x)[\"compound\"])\n",
        "    noticias_df[\"finbert_sentiment\"] = 0  # placeholder FinBERT\n",
        "\n",
        "    # Calcular promedios\n",
        "    vader_promedio = noticias_df[\"vader_sentiment\"].mean()\n",
        "    finbert_promedio = noticias_df[\"finbert_sentiment\"].mean()\n",
        "\n",
        "    # Guardar noticias procesadas\n",
        "    today_str = pd.Timestamp.utcnow().date()\n",
        "    noticias_df.to_csv(f\"logs/{symbol}_nuevas_agregadas_{today_str}.csv\", index=False)\n",
        "    registrar_log(f\"{symbol} - Guardadas {len(noticias_df)} noticias con sentiment\")\n",
        "\n",
        "    # ===========================================\n",
        "    # Actualizar DataFrame principal en memoria\n",
        "    # ===========================================\n",
        "    if df.empty:\n",
        "        df = pd.DataFrame([{\n",
        "            \"timestamp\": (pd.Timestamp.utcnow().date() - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
        "            \"vader\": vader_promedio,\n",
        "            \"finbert\": finbert_promedio,\n",
        "            \"sentiment_ratio\": vader_promedio / (finbert_promedio + 1e-9),\n",
        "            \"sentiment_combined\": (vader_promedio + finbert_promedio) / 2\n",
        "        }])\n",
        "    else:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "        df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(None)\n",
        "        df[\"date_only\"] = df[\"timestamp\"].dt.date\n",
        "        last_date = df[\"date_only\"].max()\n",
        "        last_idx = df[df[\"date_only\"] == last_date].index\n",
        "\n",
        "        if len(last_idx) == 0:\n",
        "            # Añadir nueva fila\n",
        "            new_row = {\n",
        "                \"timestamp\": (pd.Timestamp.utcnow().date() - pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
        "                \"vader\": vader_promedio,\n",
        "                \"finbert\": finbert_promedio,\n",
        "                \"sentiment_ratio\": vader_promedio / (finbert_promedio + 1e-9),\n",
        "                \"sentiment_combined\": (vader_promedio + finbert_promedio) / 2\n",
        "            }\n",
        "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        else:\n",
        "            df.loc[last_idx, \"vader\"] = vader_promedio\n",
        "            df.loc[last_idx, \"finbert\"] = finbert_promedio\n",
        "            df.loc[last_idx, \"sentiment_ratio\"] = vader_promedio / (finbert_promedio + 1e-9)\n",
        "            df.loc[last_idx, \"sentiment_combined\"] = (vader_promedio + finbert_promedio) / 2\n",
        "\n",
        "        df.drop(columns=[\"date_only\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def calcular_target_df(df, symbol=None):\n",
        "    \"\"\"\n",
        "    Calcula el target de subida para cada fila del DataFrame.\n",
        "    Los thresholds y el modo (daily/full) se leen desde config.txt\n",
        "    \"\"\"\n",
        "    # Leer thresholds y modo desde config.txt\n",
        "    target_config = {}\n",
        "    mode = \"daily\"\n",
        "    with open(\"config.txt\", \"r\") as f:\n",
        "        for line in f:\n",
        "            if \"=\" in line:\n",
        "                key, value = line.strip().split(\"=\")\n",
        "                key = key.strip()\n",
        "                value = value.strip()\n",
        "                if key in [\"high_threshold\", \"medium_threshold\", \"low_threshold\", \"below_threshold\"]:\n",
        "                    target_config[key] = float(value)\n",
        "                elif key == \"mode\":\n",
        "                    mode = value.lower()\n",
        "\n",
        "    # Función de categorización\n",
        "    def categorizar(r):\n",
        "        if pd.isna(r):\n",
        "            return None\n",
        "        elif r >= target_config[\"high_threshold\"]:\n",
        "            return \"Very High\"\n",
        "        elif r >= target_config[\"medium_threshold\"]:\n",
        "            return \"High\"\n",
        "        elif r >= target_config[\"low_threshold\"]:\n",
        "            return \"Medium\"\n",
        "        elif r >= target_config[\"below_threshold\"]:\n",
        "            return \"Low\"\n",
        "        else:\n",
        "            return \"Negative\"\n",
        "\n",
        "    # Elegir qué filas procesar\n",
        "    if mode == \"daily\":\n",
        "        idx = df.index[-1:]  # última fila\n",
        "    else:\n",
        "        idx = df.index       # todas las filas\n",
        "\n",
        "    df.loc[idx, \"target_category\"] = df.loc[idx, \"return_pct\"].apply(categorizar)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "#           EJECUCIÓN PRINCIPAL\n",
        "# ===============================================\n",
        "\n",
        "\n",
        "for symbol, info in activos.items():\n",
        "    filename = f\"{symbol}_datos.txt\"\n",
        "    df = pd.read_csv(filename, sep=\"\\t\") if os.path.exists(filename) else pd.DataFrame()\n",
        "\n",
        "    # Descargar noticias y actualizar sentimiento en memoria\n",
        "    df = descargar_noticias_y_calcular_sentiment_df(df, symbol, info[\"search_name\"])\n",
        "\n",
        "    # Calcular target usando la función que lee el modo desde activos.txt\n",
        "    df = calcular_target_df(df, symbol)\n",
        "\n",
        "    # Guardar todo el DataFrame una sola vez\n",
        "    df.to_csv(filename, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ Completed premarket (sentiment del día anterior actualizado)\")\n"
      ]
    }
  ]
}