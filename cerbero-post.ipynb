{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcvmpnNkzh7b55u9UKbfkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4L3M4R/cerbero/blob/main/cerbero_post.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96zGHOOnL0OM"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "#             CERBERO POSTMARKET\n",
        "#     Descarga precios, filtra, calcula features\n",
        "# ===============================================\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from ta.volatility import AverageTrueRange, BollingerBands\n",
        "from ta.momentum import ROCIndicator\n",
        "from ta.trend import MACD\n",
        "from ta.momentum import RSIIndicator\n",
        "import datetime\n",
        "\n",
        "# ===============================================\n",
        "#             CONFIGURACIÓN Y PARÁMETROS\n",
        "# ===============================================\n",
        "\n",
        "# Lista de activos\n",
        "activos = {}\n",
        "with open(\"activos.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        symbol, source, search_name = line.strip().split(\":\")\n",
        "        activos[symbol.strip()] = {\n",
        "            \"source\": source.strip().lower(),\n",
        "            \"search_name\": search_name.strip()\n",
        "        }\n",
        "\n",
        "# Configuración general\n",
        "config = {}\n",
        "with open(\"config.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if \":\" in line:\n",
        "            key, value = line.strip().split(\":\")\n",
        "            config[key.strip()] = value.strip()\n",
        "\n",
        "granularity = config.get(\"frecuencia\", \"1h\")\n",
        "limit = int(config.get(\"limit\", \"100\"))\n",
        "period = config.get(\"period\", \"5d\")\n",
        "\n",
        "# ===============================================\n",
        "#           FUNCIONES AUXILIARES\n",
        "# ===============================================\n",
        "\n",
        "def registrar_log(message, log_file=\"run_summary_post.log\"):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(log_file, \"a\") as log:\n",
        "        log.write(f\"[{timestamp}] {message}\\n\")\n",
        "\n",
        "def cargar_df_existente(symbol):\n",
        "    filename = f\"{symbol}_datos.txt\"\n",
        "    if os.path.exists(filename):\n",
        "        df = pd.read_csv(filename, sep=\"\\t\")\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "        df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(None)\n",
        "        existing_dates = set(df[\"timestamp\"].dt.date)\n",
        "    else:\n",
        "        df = pd.DataFrame()\n",
        "        existing_dates = set()\n",
        "    return df, existing_dates\n",
        "\n",
        "def filtrar_nuevas_fechas(df_new, existing_dates):\n",
        "    df_new[\"timestamp\"] = pd.to_datetime(df_new[\"timestamp\"], errors=\"coerce\")\n",
        "    df_new[\"timestamp\"] = df_new[\"timestamp\"].dt.tz_localize(None)\n",
        "    df_new[\"date_only\"] = df_new[\"timestamp\"].dt.date\n",
        "    df_to_add = df_new[~df_new[\"date_only\"].isin(existing_dates)].copy()\n",
        "    df_to_add.drop(columns=[\"date_only\"], inplace=True, errors=\"ignore\")\n",
        "    return df_to_add\n",
        "\n",
        "def save_df(df, symbol):\n",
        "    filename = f\"{symbol}_datos.txt\"\n",
        "    df.to_csv(filename, sep=\"\\t\", index=False)\n",
        "    print(f\"Guardado en {filename} | Filas totales: {len(df)}\")\n",
        "\n",
        "def descargar_datos_bitget(symbol, granularity, limit):\n",
        "    url = \"https://api.bitget.com/api/v2/mix/market/history-candles\"\n",
        "    params = {\"symbol\": symbol, \"productType\": \"USDT-FUTURES\", \"granularity\": granularity, \"limit\": limit}\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        registrar_log(f\"Error Bitget {symbol}: {response.text}\")\n",
        "        return None\n",
        "    data = response.json().get(\"data\", [])\n",
        "    if not data:\n",
        "        registrar_log(f\"No hay datos Bitget {symbol}\")\n",
        "        return None\n",
        "    df = pd.DataFrame(data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"quoteVolume\"])\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"].astype(int), unit='ms', utc=True)\n",
        "    df[\"symbol\"] = symbol\n",
        "    df = df.sort_values(\"timestamp\")\n",
        "    registrar_log(f\"Descargados {len(df)} registros desde Bitget para {symbol}\")\n",
        "    return df\n",
        "\n",
        "def descargar_datos_yf(symbol, interval, period, limit):\n",
        "    data = yf.download(tickers=symbol, interval=interval, period=period)\n",
        "    if data.empty:\n",
        "        registrar_log(f\"No se encontraron datos Yahoo Finance {symbol}\")\n",
        "        return None\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = [col[0].lower() for col in data.columns]\n",
        "    else:\n",
        "        data.columns = [col.lower() for col in data.columns]\n",
        "    data = data.reset_index()\n",
        "    rename_map = {'open':'open','high':'high','low':'low','close':'close','volume':'volume','Date':'timestamp'}\n",
        "    data.rename(columns=rename_map, inplace=True)\n",
        "    data[\"symbol\"] = symbol\n",
        "    data = data.head(limit)\n",
        "    registrar_log(f\"Descargados {len(data)} registros desde Yahoo Finance para {symbol}\")\n",
        "    return data\n",
        "\n",
        "# ===============================================\n",
        "#           CALCULO DE INDICADORES\n",
        "# ===============================================\n",
        "\n",
        "def calcular_features(df):\n",
        "    for col in ['open','high','low','close','volume']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df['gap_apertura_pct'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1) * 100\n",
        "    df['volatilidad_diaria'] = (df['high'] - df['low']) / df['low'] * 100\n",
        "    df['vol_5d'] = df['volatilidad_diaria'].rolling(5).std()\n",
        "    df['vol_10d'] = df['volatilidad_diaria'].rolling(10).std()\n",
        "    df['vol_rel_5d'] = df['volume'] / df['volume'].rolling(5).mean()\n",
        "    df['return_pct'] = (df['close'] - df['close'].shift(1)) / df['close'].shift(1) * 100\n",
        "    df['close_open_pct'] = (df['close'] - df['open']) / df['open'] * 100\n",
        "    df['range_pct'] = (df['high'] - df['low']) / df['low'] * 100\n",
        "    df['volume_change_pct'] = (df['volume'] - df['volume'].shift(1)) / df['volume'].shift(1) * 100\n",
        "\n",
        "    df['rsi_14'] = RSIIndicator(close=df['close'], window=14).rsi()\n",
        "    macd = MACD(close=df['close'], window_slow=26, window_fast=12, window_sign=9)\n",
        "    df['macd'] = macd.macd()\n",
        "    df['macd_signal'] = macd.macd_signal()\n",
        "    df['macd_diff'] = macd.macd_diff()\n",
        "    df['atr_14'] = AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=14).average_true_range()\n",
        "    df['momentum_12'] = ROCIndicator(close=df['close'], window=12).roc()\n",
        "    bb = BollingerBands(close=df['close'], window=20, window_dev=2)\n",
        "    df['bb_upper'] = bb.bollinger_hband()\n",
        "    df['bb_lower'] = bb.bollinger_lband()\n",
        "    df['bb_pctb'] = bb.bollinger_pband()\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "# ===============================================\n",
        "#           PROCESAMIENTO POR ACTIVO\n",
        "# ===============================================\n",
        "\n",
        "def procesar_activo(symbol, source, search_name, granularity, period, limit):\n",
        "    df_existing, existing_dates = cargar_df_existente(symbol)\n",
        "\n",
        "    df_new = descargar_datos_bitget(symbol, granularity, limit) if source==\"bitget\" else descargar_datos_yf(symbol, granularity, period, limit)\n",
        "\n",
        "    if df_new is None or df_new.empty:\n",
        "        registrar_log(f\"{symbol} - No se descargaron datos\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "    df_new[\"download_date\"] = pd.Timestamp.utcnow()\n",
        "    df_new.to_csv(f\"logs/{symbol}_descargadas_{pd.Timestamp.utcnow().date()}.csv\", index=False)\n",
        "\n",
        "    df_to_add = filtrar_nuevas_fechas(df_new, existing_dates)\n",
        "    if df_to_add.empty:\n",
        "        registrar_log(f\"{symbol} - No hay nuevas filas\")\n",
        "        return\n",
        "\n",
        "    df_to_add[\"added_date\"] = pd.Timestamp.utcnow()\n",
        "    df_to_add.to_csv(f\"logs/{symbol}_nuevas_agregadas_{pd.Timestamp.utcnow().date()}.csv\", index=False)\n",
        "\n",
        "    df_final = pd.concat([df_existing, df_to_add], ignore_index=True) if not df_existing.empty else df_to_add\n",
        "    df_final = calcular_features(df_final)\n",
        "    save_df(df_final, symbol)\n",
        "\n",
        "    registrar_log(f\"{symbol} - Descargadas {len(df_new)} filas, agregadas {len(df_to_add)}, total histórico {len(df_final)}\")\n",
        "\n",
        "# ===============================================\n",
        "#           EJECUCIÓN PRINCIPAL\n",
        "# ===============================================\n",
        "\n",
        "for symbol, info in activos.items():\n",
        "    procesar_activo(symbol, info[\"source\"], info[\"search_name\"], granularity, period, limit)\n",
        "\n",
        "print(\"Completed postmarket\")\n"
      ]
    }
  ]
}
